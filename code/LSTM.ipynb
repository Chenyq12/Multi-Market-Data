{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e6afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86153\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.utils.data as Data\n",
    "import torch\n",
    "from torch.nn import Module, LSTM, Linear\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from function import LSTM_Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b46576",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df02601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_data = pd.read_csv('../data/date.csv')\n",
    "# read_data = read_data.set_index('Date')\n",
    "# read_data = read_data.loc['2014-01-01':]\n",
    "# # read_data\n",
    "# fulldata = read_data[[\n",
    "# #                     'AS51',                         #0.431\n",
    "# #                  'CKLSE',                       #0.265\n",
    "# #                  'CSX5P',                   #0.318\n",
    "# #                  'DJI',                 #0.43\n",
    "# #                  'FCHI',            #0.17\n",
    "# #                  'FTSE',        #0.58\n",
    "# #                  'GDAXI',   #0.391\n",
    "# #                  'HSI',      #0.374\n",
    "# #                  'IBOVESPA',   #0.298\n",
    "#                  'IXIC',         #0.524\n",
    "# #                  'KS11',           #0.26\n",
    "# #                  'N225',            #0.621\n",
    "# #                  'RUT',              #0.6\n",
    "# #                  'SENSEX',            #0.282\n",
    "# #                  'SPTSX',               #0.39\n",
    "# #                  'SPX',                   #0.675\n",
    "# #                  'TWII',                    #0.3\n",
    "# #                  'shanghai',                    #0.074\n",
    "#                  'shenzheng']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd57f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_data = pd.read_csv('../data/bond/date.csv')\n",
    "# read_data = read_data.set_index('Date')\n",
    "# read_data = read_data.loc['2014-01-01':]\n",
    "# fulldata = read_data[[\n",
    "# #                         'Australia',\n",
    "# #                       'Brazil',\n",
    "# #                       'Canada',\n",
    "#                         # 'Germany',\n",
    "# #                         'France',\n",
    "# #                       'Hongkong',\n",
    "# #                       'India',\n",
    "# #                       'Japan',\n",
    "#                       'Malaysia',\n",
    "# #                       'US' ,\n",
    "#                       'Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fedb1831",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_data = pd.read_csv('../data/rate/date.csv')\n",
    "read_data = read_data.set_index('Date')\n",
    "read_data = read_data.loc['2014-01-01':]\n",
    "fulldata = read_data[[\n",
    "#                         'Australia',\n",
    "#                        'Brazil',\n",
    "#                       'Canada',\n",
    "#                       'EU',\n",
    "#                       'Hongkong',\n",
    "#                       'India', \n",
    "#                       'Japan',\n",
    "#                       'Korea',\n",
    "#                       'Malaysia',\n",
    "#                       'Taiwan', \n",
    "                      'UK',\n",
    "#                       'US' ,\n",
    "                      'Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283430ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = fulldata[['Close']]\n",
    "# fulldata = pd.concat([a, a], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454bc89",
   "metadata": {},
   "source": [
    "# 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9491af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_day = 1             # 预测未来几天\n",
    "time_step = 25              # 这个参数很重要，是设置用前多少天的数据来预测，也是LSTM的time step数，请保证训练数据量大于它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdead050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #超参数\n",
    "# batch_size = 32\n",
    "# hidden_size = 128           # LSTM的隐藏层大小，也是输出大小\n",
    "# lstm_layers = 2             # LSTM的堆叠层数\n",
    "# dropout_rate = 0.001          # dropout概率\n",
    "# learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb4bd8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = len(fulldata.columns.tolist())\n",
    " # 数据参数\n",
    "if(c>2):\n",
    "    feature_columns = list(range(0, c-1))     # 要作为feature的列，按原数据从0开始计算，也可以用list 如 [2,4,6,8] 设置\n",
    "    label_columns = [c-1]                  # 要预测的列，按原数据从0开始计算, 如同时预测第四，五列 最低价和最高价\n",
    "else:\n",
    "    feature_columns = [0]\n",
    "    label_columns = [1]\n",
    "\n",
    "shuffle_train_data = True   # 是否对训练数据做shuffle\n",
    "random_seed = 32            # 随机种子，保证可复现\n",
    "\n",
    "train_data_rate = 0.80      # 训练数据占总体数据比例，测试数据就是 1-train_data_rate\n",
    "valid_data_rate = 0.20      # 验证数据占训练数据比例，验证集在训练过程使用，为了做模型和参数选择\n",
    "\n",
    "# do_continue_train = False    # 每次训练把上一次的final_state作为下一次的init_state，仅用于RNN类型模型，目前仅支持pytorch\n",
    "do_continue_train = True \n",
    "start_num_in_test = 0      # 测试集中前几天的数据会被删掉，因为它不够一个time_step\n",
    "\n",
    "data, data_column_name = fulldata.values, fulldata.columns.tolist()\n",
    "data_num = data.shape[0]\n",
    "train_num = int(data_num * train_data_rate)\n",
    "std = np.std(data, axis=0)\n",
    "mean = np.mean(data, axis=0)              # 数据的均值和方差\n",
    "norm_data = (data - mean)/std   # 归一化，去量纲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729d109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数范围\n",
    "parameters = {\n",
    "    'Batch_size': [32, 64],\n",
    "    'lstm_layers': [1,2,4],\n",
    "    'hidden_size': [64,128,256],\n",
    "    'Learning_rate': [0.001,0.01],\n",
    "    'dropout_rate': [0.001,0.01]\n",
    "}\n",
    "\n",
    "# 生成所有可能的参数组合\n",
    "parameter_combinations = []\n",
    "for batch_size in parameters['Batch_size']:\n",
    "    for hidden_layer in parameters['lstm_layers']:\n",
    "        for neurons in parameters['hidden_size']:\n",
    "            for learning_rate in parameters['Learning_rate']:\n",
    "                for dropout_rate in parameters['dropout_rate']:\n",
    "                    parameter_combinations.append({\n",
    "                        'Batch_size': batch_size,\n",
    "                        'lstm_layers': hidden_layer,\n",
    "                        'hidden_size': neurons,\n",
    "                        'Learning_rate': learning_rate,\n",
    "                        'dropout_rate': dropout_rate\n",
    "                })\n",
    "\n",
    "# #打印所有参数组合\n",
    "# for idx, params in enumerate(parameter_combinations):\n",
    "#     print(f\"参数组合 {idx+1}: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378cf6da",
   "metadata": {},
   "source": [
    "# 初始化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef38298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_valid_data():\n",
    "#     feature_data = norm_data[:train_num, feature_columns]\n",
    "    feature_data = norm_data[:train_num]\n",
    "    label_data = norm_data[time_step : predict_day + train_num,\n",
    "                                    label_columns]    # 将延后几天的数据作为label\n",
    "\n",
    "    \n",
    "       # 在非连续训练模式下，每time_step行数据会作为一个样本，两个样本错开一行，比如：1-20行，2-21行。。。。\n",
    "    train_x = [feature_data[i:i+time_step] for i in range(train_num-time_step)]\n",
    "    train_y = [label_data[i:i+predict_day] for i in range(train_num-time_step)]\n",
    "\n",
    "    train_x, train_y = np.array(train_x), np.array(train_y)\n",
    "\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=valid_data_rate,\n",
    "                                                              random_state=random_seed,\n",
    "                                                              shuffle=shuffle_train_data)   # 划分训练和验证集，并打乱\n",
    "    \n",
    "    train_X, valid_X, train_Y, valid_Y = train_x, valid_x, train_y, valid_y\n",
    "    train_and_valid_data=[train_X, train_Y, valid_X, valid_Y]\n",
    "\n",
    "    train_X, train_Y, valid_X, valid_Y = train_and_valid_data\n",
    "    train_X, train_Y = torch.from_numpy(train_X).float(), torch.from_numpy(train_Y).float()     # 先转为Tensor\n",
    "    train_loader = DataLoader(TensorDataset(train_X, train_Y), batch_size=batch_size)    # DataLoader可自动生成可训练的batch数据\n",
    "\n",
    "    valid_X, valid_Y = torch.from_numpy(valid_X).float(), torch.from_numpy(valid_Y).float()\n",
    "    valid_loader = DataLoader(TensorDataset(valid_X, valid_Y), batch_size=batch_size)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06540080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_data(pred_result, test_Y):\n",
    "    # 假设这是数据还原的函数\n",
    "    # 在这里实现将归一化后的数据还原成原始数据的过程\n",
    "    # 返回还原后的数据\n",
    "    std1 = std[1]\n",
    "    mean1 = mean[1]\n",
    "    true = test_Y*std1 + mean1\n",
    "    pred = pred_result*std1 + mean1\n",
    "    return pred, true\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer):\n",
    "    train_loss_array = []\n",
    "    train_loss = []\n",
    "    for i, _data in enumerate(train_loader):\n",
    "        _train_X, _train_Y = _data[0],_data[1]\n",
    "        hidden_train = None  # 手动创建新的隐藏状态\n",
    "        optimizer.zero_grad()               # 训练前要将梯度信息置 0\n",
    "        pred_Y, hidden_train = model(_train_X, hidden_train)    # 这里走的就是前向计算forward函数\n",
    "        h_0, c_0 = hidden_train\n",
    "        h_0.detach_(), c_0.detach_()    # 去掉梯度信息\n",
    "        hidden_train = (h_0, c_0)\n",
    "        # 假设你的目标张量为 target\n",
    "        _train_Y = _train_Y.squeeze(dim=2)\n",
    "        pred_Y = pred_Y[:,-1]\n",
    "        _train_Y = _train_Y[:,-1]\n",
    "        \n",
    "        loss = criterion(pred_Y, _train_Y)  # 计算loss\n",
    "        loss.backward()                     # 将loss反向传播\n",
    "        optimizer.step()                    # 用优化器更新参数\n",
    "        train_loss_array.append(loss.item())\n",
    "    train_loss.append(np.mean(train_loss_array))\n",
    "    train_loss_cur = np.mean(train_loss_array)\n",
    "    return train_loss_cur\n",
    "\n",
    "def eval(model, dataloader):\n",
    "  # 先定义一个tensor保存预测结果\n",
    "    result_valid = torch.Tensor()\n",
    "    true_valid = torch.Tensor()\n",
    "    model.eval()                    # pytorch中，预测时要转换成预测模式\n",
    "    valid_loss_array = []\n",
    "    hidden_valid = None\n",
    "    val_loss = []\n",
    "    \n",
    "    for _valid_X, _valid_Y in valid_loader:\n",
    "        # 使用模型的初始隐藏状态\n",
    "        hidden_valid = None \n",
    "        pred_Y, hidden_valid = model(_valid_X, hidden_valid)\n",
    "        _valid_Y = _valid_Y.squeeze(dim=2)\n",
    "        pred_Y = pred_Y[:,-1]\n",
    "        _valid_Y = _valid_Y[:,-1]\n",
    "        loss = criterion(pred_Y, _valid_Y)  # 验证过程只有前向计算，无反向传播过程\n",
    "        valid_loss_array.append(loss.item())\n",
    "        \n",
    "        cur_pred = torch.squeeze(_valid_Y, dim=0)\n",
    "        true_valid = torch.cat((true_valid, cur_pred), dim=0)\n",
    "        \n",
    "        cur_pred = torch.squeeze(pred_Y, dim=0)\n",
    "        result_valid = torch.cat((result_valid, cur_pred), dim=0)\n",
    "    true_valid = true_valid.detach().numpy() \n",
    "    result_valid = result_valid.detach().numpy()    \n",
    "        \n",
    "    val_loss.append(np.mean(valid_loss_array))\n",
    "    valid_loss_cur = np.mean(valid_loss_array)\n",
    "    \n",
    "    return valid_loss_cur, true_valid, result_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de878b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14d33e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.001 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.01 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.001 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 2\n",
      "72 3\n",
      "72 4\n",
      "72 5\n",
      "72 6\n",
      "72 7\n",
      "72 8\n",
      "72 9\n",
      "72 10\n",
      "72 11\n",
      "72 12\n",
      "72 13\n",
      "72 14\n",
      "72 15\n",
      "72 16\n",
      "72 17\n",
      "72 18\n",
      "72 19\n",
      "72 20\n",
      "72 21\n",
      "72 22\n",
      "72 23\n",
      "72 24\n",
      "72 25\n",
      "72 26\n",
      "72 27\n",
      "72 28\n",
      "72 29\n",
      "72 30\n",
      "72 31\n",
      "72 32\n",
      "72 33\n",
      "72 34\n",
      "72 35\n",
      "72 36\n",
      "72 37\n",
      "72 38\n",
      "72 39\n",
      "72 40\n",
      "72 41\n",
      "72 42\n",
      "72 43\n",
      "72 44\n",
      "72 45\n",
      "72 46\n",
      "72 47\n",
      "72 48\n",
      "72 49\n",
      "72 50\n",
      "72 51\n",
      "72 52\n",
      "72 53\n",
      "72 54\n",
      "72 55\n",
      "72 56\n",
      "72 57\n",
      "72 58\n",
      "72 59\n",
      "72 60\n",
      "72 61\n",
      "72 62\n",
      "72 63\n",
      "72 64\n",
      "72 65\n",
      "72 66\n",
      "72 67\n",
      "72 68\n",
      "72 69\n",
      "72 70\n",
      "72 71\n"
     ]
    }
   ],
   "source": [
    " # 网络参数\n",
    "# input_size = norm_data.shape[1]-1\n",
    "input_size = norm_data.shape[1]\n",
    "output_size = predict_day\n",
    "\n",
    "results = []\n",
    "COUNT = len(parameter_combinations)\n",
    "\n",
    "\n",
    "for i in range(len(parameter_combinations)):\n",
    "# for i in range(3):\n",
    "        # 假设你想选择第三个参数组合\n",
    "    selected_params = parameter_combinations[i]\n",
    "\n",
    "    # 然后你就可以按照键来获取对应的值\n",
    "    batch_size = selected_params['Batch_size']\n",
    "    lstm_layers = selected_params['lstm_layers']\n",
    "    hidden_size = selected_params['hidden_size']\n",
    "    learning_rate = selected_params['Learning_rate']\n",
    "    dropout_rate = selected_params['dropout_rate']\n",
    "    \n",
    "\n",
    "    \n",
    "    train_loader, valid_loader = get_train_and_valid_data()\n",
    "    \n",
    "    model = LSTM_Net(input_size, hidden_size, output_size, lstm_layers, dropout_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()      # 这两句是定义优化器和loss\n",
    "    best_loss = 5\n",
    "    epochs = 100 # 整个训练集被训练多少遍，不考虑早停的前提下\n",
    "    patience = 40               # 训练多少epoch，验证集没提升就停掉\n",
    "    bad_epoch = 0\n",
    "    vl_loss = []\n",
    "    for epoch_i in range(epochs):\n",
    "        train_loss = train(model, train_loader, optimizer)\n",
    "\n",
    "        eval_loss, tru, pre = eval(model, valid_loader)\n",
    "\n",
    "        vl_loss.append(eval_loss)\n",
    "        if eval_loss < best_loss:\n",
    "            best_loss = eval_loss\n",
    "            best_epoch = epoch_i\n",
    "            best_result_valid = pre\n",
    "            best_true_valid = tru\n",
    "    #         torch.save(model.state_dict(), '../model/best_ANN_trainModel.pth')  # 模型保存\n",
    "        else:\n",
    "            bad_epoch += 1\n",
    "            if bad_epoch >= patience:    # 如果验证集指标连续patience个epoch没有提升，就停掉训练\n",
    "                break\n",
    "    results.append({\n",
    "                    'epoch': best_epoch,\n",
    "                    'loss': best_loss})\n",
    "    print(COUNT,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e22cc399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数组合 1: {'epoch': 61, 'loss': 0.007269594779548545}\n",
      "参数组合 2: {'epoch': 76, 'loss': 0.007409100731213887}\n",
      "参数组合 3: {'epoch': 9, 'loss': 0.008173541515134275}\n",
      "参数组合 4: {'epoch': 8, 'loss': 0.007968709610092143}\n",
      "参数组合 5: {'epoch': 56, 'loss': 0.00759950610032926}\n",
      "参数组合 6: {'epoch': 48, 'loss': 0.007512958642716209}\n",
      "参数组合 7: {'epoch': 50, 'loss': 0.008405869904284676}\n",
      "参数组合 8: {'epoch': 36, 'loss': 0.00899818679317832}\n",
      "参数组合 9: {'epoch': 89, 'loss': 0.007059343042783439}\n",
      "参数组合 10: {'epoch': 85, 'loss': 0.0070414597090954585}\n",
      "参数组合 11: {'epoch': 31, 'loss': 0.007378391611079375}\n",
      "参数组合 12: {'epoch': 40, 'loss': 0.008650517906062305}\n",
      "参数组合 13: {'epoch': 66, 'loss': 0.0075106459359327955}\n",
      "参数组合 14: {'epoch': 28, 'loss': 0.0075140963308513165}\n",
      "参数组合 15: {'epoch': 38, 'loss': 0.008397321760033568}\n",
      "参数组合 16: {'epoch': 41, 'loss': 0.008506667916662991}\n",
      "参数组合 17: {'epoch': 18, 'loss': 0.007978076542106768}\n",
      "参数组合 18: {'epoch': 19, 'loss': 0.00815808707072089}\n",
      "参数组合 19: {'epoch': 56, 'loss': 0.007906260415135572}\n",
      "参数组合 20: {'epoch': 49, 'loss': 0.008632638957351446}\n",
      "参数组合 21: {'epoch': 54, 'loss': 0.009711475732425848}\n",
      "参数组合 22: {'epoch': 54, 'loss': 0.009864684388351938}\n",
      "参数组合 23: {'epoch': 32, 'loss': 0.008827822554546097}\n",
      "参数组合 24: {'epoch': 48, 'loss': 0.007989205051368723}\n",
      "参数组合 25: {'epoch': 18, 'loss': 0.0075189330576298135}\n",
      "参数组合 26: {'epoch': 20, 'loss': 0.00761974536969016}\n",
      "参数组合 27: {'epoch': 36, 'loss': 0.007794652949087322}\n",
      "参数组合 28: {'epoch': 26, 'loss': 0.008963660608666638}\n",
      "参数组合 29: {'epoch': 21, 'loss': 0.008119030894401172}\n",
      "参数组合 30: {'epoch': 17, 'loss': 0.009534968024430176}\n",
      "参数组合 31: {'epoch': 0, 'loss': 0.5430680364370346}\n",
      "参数组合 32: {'epoch': 38, 'loss': 1.2105013976494472}\n",
      "参数组合 33: {'epoch': 59, 'loss': 0.00802210004379352}\n",
      "参数组合 34: {'epoch': 48, 'loss': 0.007968940151234468}\n",
      "参数组合 35: {'epoch': 52, 'loss': 0.13622563456495604}\n",
      "参数组合 36: {'epoch': 1, 'loss': 1.2106582621733348}\n",
      "参数组合 37: {'epoch': 95, 'loss': 0.007417916941146056}\n",
      "参数组合 38: {'epoch': 88, 'loss': 0.00736124146108826}\n",
      "参数组合 39: {'epoch': 35, 'loss': 0.007300322176888585}\n",
      "参数组合 40: {'epoch': 82, 'loss': 0.007091796801735957}\n",
      "参数组合 41: {'epoch': 83, 'loss': 0.007443465292453766}\n",
      "参数组合 42: {'epoch': 79, 'loss': 0.0073643638752400875}\n",
      "参数组合 43: {'epoch': 58, 'loss': 0.007194696847970287}\n",
      "参数组合 44: {'epoch': 39, 'loss': 0.007075757409135501}\n",
      "参数组合 45: {'epoch': 77, 'loss': 0.007193408906459808}\n",
      "参数组合 46: {'epoch': 66, 'loss': 0.007240998093038797}\n",
      "参数组合 47: {'epoch': 62, 'loss': 0.0074038140786190825}\n",
      "参数组合 48: {'epoch': 61, 'loss': 0.007230468948061268}\n",
      "参数组合 49: {'epoch': 58, 'loss': 0.007549687754362822}\n",
      "参数组合 50: {'epoch': 53, 'loss': 0.007430435235922535}\n",
      "参数组合 51: {'epoch': 66, 'loss': 0.006932685462137063}\n",
      "参数组合 52: {'epoch': 51, 'loss': 0.007318866439163685}\n",
      "参数组合 53: {'epoch': 43, 'loss': 0.007681050415461262}\n",
      "参数组合 54: {'epoch': 74, 'loss': 0.007183524547144771}\n",
      "参数组合 55: {'epoch': 53, 'loss': 0.007385053982337316}\n",
      "参数组合 56: {'epoch': 52, 'loss': 0.007307006123786171}\n",
      "参数组合 57: {'epoch': 36, 'loss': 0.007445889292284846}\n",
      "参数组合 58: {'epoch': 33, 'loss': 0.00764204243508478}\n",
      "参数组合 59: {'epoch': 21, 'loss': 0.006951153045520186}\n",
      "参数组合 60: {'epoch': 21, 'loss': 0.007680600276216865}\n",
      "参数组合 61: {'epoch': 42, 'loss': 0.00788285699672997}\n",
      "参数组合 62: {'epoch': 55, 'loss': 0.0075879700016230345}\n",
      "参数组合 63: {'epoch': 21, 'loss': 0.008681130440284809}\n",
      "参数组合 64: {'epoch': 35, 'loss': 0.00756158446893096}\n",
      "参数组合 65: {'epoch': 67, 'loss': 0.007922259469827017}\n",
      "参数组合 66: {'epoch': 56, 'loss': 0.007848869155471524}\n",
      "参数组合 67: {'epoch': 43, 'loss': 0.00814087688922882}\n",
      "参数组合 68: {'epoch': 46, 'loss': 0.007752344633142154}\n",
      "参数组合 69: {'epoch': 37, 'loss': 0.00760956690646708}\n",
      "参数组合 70: {'epoch': 39, 'loss': 0.007641329507653912}\n",
      "参数组合 71: {'epoch': 57, 'loss': 0.1070624366402626}\n",
      "参数组合 72: {'epoch': 51, 'loss': 0.15548047175010046}\n"
     ]
    }
   ],
   "source": [
    "for idx, params in enumerate(results):\n",
    "    print(f\"参数组合 {idx+1}: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d55d7479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并参数组合和结果到一个字典列表\n",
    "combined_params = []\n",
    "for params, result in zip(parameter_combinations, results):\n",
    "    combined_params.append({**params, **result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "724c65e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Three Parameters:\n",
      "Rank 1: {'Batch_size': 64, 'lstm_layers': 2, 'hidden_size': 64, 'Learning_rate': 0.01, 'dropout_rate': 0.001, 'epoch': 66, 'loss': 0.006932685462137063}\n",
      "Rank 2: {'Batch_size': 64, 'lstm_layers': 2, 'hidden_size': 256, 'Learning_rate': 0.01, 'dropout_rate': 0.001, 'epoch': 21, 'loss': 0.006951153045520186}\n",
      "Rank 3: {'Batch_size': 32, 'lstm_layers': 1, 'hidden_size': 256, 'Learning_rate': 0.001, 'dropout_rate': 0.01, 'epoch': 85, 'loss': 0.0070414597090954585}\n",
      "Rank 4: {'Batch_size': 32, 'lstm_layers': 1, 'hidden_size': 256, 'Learning_rate': 0.001, 'dropout_rate': 0.001, 'epoch': 89, 'loss': 0.007059343042783439}\n",
      "Rank 5: {'Batch_size': 64, 'lstm_layers': 1, 'hidden_size': 128, 'Learning_rate': 0.01, 'dropout_rate': 0.01, 'epoch': 39, 'loss': 0.007075757409135501}\n",
      "Rank 6: {'Batch_size': 64, 'lstm_layers': 1, 'hidden_size': 64, 'Learning_rate': 0.01, 'dropout_rate': 0.01, 'epoch': 82, 'loss': 0.007091796801735957}\n",
      "Rank 7: {'Batch_size': 64, 'lstm_layers': 2, 'hidden_size': 128, 'Learning_rate': 0.001, 'dropout_rate': 0.01, 'epoch': 74, 'loss': 0.007183524547144771}\n",
      "Rank 8: {'Batch_size': 64, 'lstm_layers': 1, 'hidden_size': 256, 'Learning_rate': 0.001, 'dropout_rate': 0.001, 'epoch': 77, 'loss': 0.007193408906459808}\n",
      "Rank 9: {'Batch_size': 64, 'lstm_layers': 1, 'hidden_size': 128, 'Learning_rate': 0.01, 'dropout_rate': 0.001, 'epoch': 58, 'loss': 0.007194696847970287}\n",
      "Rank 10: {'Batch_size': 64, 'lstm_layers': 1, 'hidden_size': 256, 'Learning_rate': 0.01, 'dropout_rate': 0.01, 'epoch': 61, 'loss': 0.007230468948061268}\n"
     ]
    }
   ],
   "source": [
    "# 根据损失值排序\n",
    "sorted_params = sorted(combined_params, key=lambda x: x['loss'])\n",
    "\n",
    "# 取前三个\n",
    "top_three_params = sorted_params[:10]\n",
    "\n",
    "print(\"Top Three Parameters:\")\n",
    "for i, param in enumerate(top_three_params, start=1):\n",
    "    print(f\"Rank {i}: {param}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
